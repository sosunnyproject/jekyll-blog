---
layout: post
categories: dev
title: "딥러닝 cs231n 번역 겸 공부"
date: 2018-03-10T13:01:27-05:00
share: false
---

> Translating http://cs231n.github.io/convolutional-networks/ to Korean 스탠포드 강의/숙제 설명 한국어 번역중

용어 정리
- receptive field: (필터 사이즈와 같은 거지, 필터 자체가 아님.. `맞나요..?`) 뉴런과 인풋의 로컬영역을 연결할 때, 그 연결고리의 spatial 범위인 하이퍼파라미터
- spatially: width x height 면적에 대해서
- spatial dimensions: widthxheight 차원
- depth dimension: 깊이 차원
- volume: width x height x depth
- nonlinear layer = activation layer
- depth = 총 Conv의 깊이가 아니라 액티베이션 볼륨의 3번째 차원을 의미한다.


# Convolutional Neural Networks (CNNs/ConvNets)

CNN은 우리가 이전 장에서 본 일반적인 뉴럴 네트워크와 흡사하다. CNN은 학습하며 업데이트 되는 편향치(bias)와 가중치(weight)를 가진 뉴런으로 구성되어 있다. 

각 뉴런은 몇가지의 인풋을 받은 후, 행렬의 내적(dot product)를 수행하고, 원한다면 non-linearity를 취해줄 수 있다. 이 뉴런들로 큰 하나의 네트워크를 만든다 할지라도, 네트워크는 여전히 하나의(single) 미분가능한 스코어 함수이다: 한쪽은 원본 이미지 픽셀, 다른쪽은 클래스 스코어. 더불어, CNN에서도 마지막 레이어 (fc layer)에 손실 함수 (svm, softmax 등)를 적용한다. 일반적인 뉴럴 네트워크에 썼던 팁/트릭/기법들을 거의 그대로 적용할 수 있다고 생각하면 된다. 

그럼 뭐가 다른걸까? ConvNet 구조에서는 한가지 확실한 가정을 해두고 가야한다. "인풋은 이미지이다. 그리고 우리는 그 이미지들의 특정 속성들을 구조로 인코딩할 수 있어야 한다." 이게 가능해야지 우리가 forward function 을 더 효율적으로 실행할 수 있고, 네트워크의 수많은 파라미터를 확 줄일 수 있다. 

## 구조 훑어보기

#### _일반적인 뉴럴 넷 (신경망)을 기억해보자._
우리가 이전 장에서 보았던 것처럼, 뉴럴 네트워크는 벡터 하나를 인풋으로 받고, 그것을 여러 hidden 레이어들에 통과시켜서 변형한다. 각 hidden 레이어는 여러 뉴런들의 집합으로 구성되어 있다. 이 때 이 hidden 레이어의 뉴런 하나하나는 그 전 레이어의 뉴런들에 모두 빠짐없이 연결되어 있다. 그리고 같은 레이어 안에 있는 뉴런들끼리는 전혀 연결되어 있지 않다. 마지막 fully-connected 레이어는 'output layer' (결과값 레이어)로 불리며, 분류 문제(classification)에서는 이 최종 레이어가 클래스 스코어를 나타낸다. 

일반적인 뉴럴 넷은 큰 이미지 전체 사이즈로 잘 확장할 수가 없다. CIFAR-10에서 이미지들은 32x32x3 (32폭, 32높이, 3 칼라 채널)사이즈 정도 밖에 안된다. 그래서 첫 hidden 레이어에 있는 한개의 뉴런 (fully-connected neuron: 아까 위에서 뉴런은 그 전 애들과 다 연결되어 있다고 했으니까)은 32x32x3 = 3072 개의 가중치를 갖는다. 이 정도면 감당할 수 있을 것처럼 보이는가? 하지만 이런 fully-connected 구조 `폭x높이x칼라 다 곱하고 뉴런끼리 다 이어주는 구조` 는 더 큰 이미지 사이즈에는 적용할 수가 없다. 예를 들어, 좀 더 흔히 사용되는 이미지 사이즈는 더 클 것이고, 200x200x3이라고 가정해보자. 그렇다면 200x200x3 = 120,000개의 가중치를 갖는 뉴런이 생기고, 그러한 뉴런들이 모인다면 순식간에 파라미터 개수는 불어날 것이다. 즉, 이러한 full connectivity 는 소모적이며, 수많은 파라미터는 멀지않아 오버피팅을 자초할 것이다.

#### _3차원 볼륨의 뉴런들_

CNN은 _인풋이 이미지로 이루어져 있고 이미지들은 나름 합리적으로 구조를 이루고 있다_ 는 점을 유리하게 이용한다. 일반적인 뉴럴 네트워크와 다르게, ConvNet의 뉴런들은 3차원에 배열되어 있다: 폭, 넓이, 깊이 (여기서 깊이는 활성 volume의 3번째 차원이며, 전체 신경망의 깊이/네트워크의 전체 레이어 개수가 아니다). 예를 들어, CIFAR-10에서 구해오는 인풋 이미지들은 **input volume of activation** 이며, volume은 32x32x3 의 차원을 가지고 있다. 잠시 후에 같이 보겟지만, 레이어의 뉴런들은 이전 레이어에 부분적으로만 연결될 것이다. 그전처럼 fully-connected 되는 것이 아니라. 나아가, CIFAR-10 
을 인풋으로 돌린 네트워크의 마지막 최종 레이어는 1x1x10 차원을 가지게 된다. 왜냐하면 ConvNet 구조의 끝에서는 이미지 통짜 하나를 스코어 클래스를 나타내는 벡터 하나로 줄여버리기 때문이다. 이 때 이 단일 벡터는 depth 차원을 따라서 배열되어 있는 형태이다. `depth 차원의 사이즈를 따른다는 의미로 해석하면 될 것 같다.`

![이미지](http://cs231n.github.io/assets/nn1/neural_net2.jpeg)

> 좌: 일반적인 3층의 신경망 구조. 

> 우: ConvNet 는 자기 뉴런들을 3차원으로 배열한다 (폭, 넓이, 깊이). ConvNet의 각 레이어는 3D 인풋 볼륨을 뉴런들이  활성화된 3D 아웃풋 볼륨으로 바꿔준다. 이 예시에서, 빨강 인풋 레이어가 이미지이고, 빨강의 폭과 넓이는 이미지의 사이즈이다. 그리고 깊이는 빨, 초, 파 칼라 채널이다. 

>> ConvNet은 레이어로 이루어져 있다. 각 레이어는 간단한 API이다. 3차원 인풋을 3차원 아웃풋으로 바꿔준다. 파라미터를 가지고 있을 수도, 아닐 수도 있는, 몇몇 미분 가능한 함수를 이용해서.

## Layers used to build ConvNets ConvNets 만드는 데 필요한 레이어들
위에서 설명했듯이, 간단한 ConvNet (CNN 지칭)은 레이어들의 연속이다. 그리고 각 레이어는 미분 함수를 통해서 _활성화의 한 볼륨_ 을 또다른 것으로 변환시킨다. 우리는 3개의 주요 레이어 타입을 ConvNet 구조에 사용한다: Convolutional Layer, Pooling layer, Fully Connected Layer (바로 일반적인 신경망 네트워크에서 사용하였듯이). 우리는 이런 레이어들을 쌓아서 완전한 ConvNet 구조를 만들 것이다.

**구조 예시: 훑어보기**

아래에서 더 자세히 살펴보겠지만, CIFAR-10 분류를 위한 간단한 ConvNet의 구조는 이렇게 되시겠다: 인풋 - Conv - Relu - Pool - Fc.

자세한 설명:
- 인풋 [32x32x3] 은 이미지의 raw 픽셀값들을 가지고 있다. 이 경우, 폭 32, 높이 32, 색 채널 3이다.
- ConV 레이어는 뉴런들의 아웃풋을 계산한다. 뉴런들은 인풋의 로컬 영역에 연결되어 있고, 각 뉴런은 그들의 가중치와 인풋 로컬영역 간의 행렬 내적곱을 계산한다. 그래서 우리가 만약 12개의 필터를 사용한다치면 [32x32x12] 사이즈의 볼륨을 얻을 것이다.
- RELU 레이어는 elementwise (원소별) 활성화 함수를 사용하는데, max(0,x)계산은 0을 경계점으로 만들어버린다. 이 때, 볼륨의 사이즈(32x32x12)는 바뀌지 않는다.
- 풀링 레이어는 이미지 사이즈를 줄여주는 역할을 한다. 폭과 넓이로 이뤄진 공간 차원을 따라서 사이즈를 축소시켜서 32x32x12를 16x16x12로 바꾸듯이 더 작게 만들어준다.
- FC 레이어는 클래스 스코어를 계산해서 1x1x10 사이즈의 볼륨을 생성한다. CIFAR-10에서 10가지 카테고리가 있는 것처럼, 10개의 숫자들은 클래스 스코어에 해당한다. 일반 뉴럴 네트워크에서 하던 것처럼 (그리고 fully-connected라는 이름이 암시하듯이) 이 레이어의 각 뉴런은 이전 볼륨의 모든 숫자에 연결되어 있다.

이러한 방식으로 ConveNet들은 레이어 하나하나를 거치며 원본 이미지 픽셀 값을 최종 클래스 스코어로 변환한다. 단, 어떤 레이어는 파라미터를 가지고 있고 어떤 것은 그렇지 않다는 것을 주의해라. 특히, ConV/FC 레이어들은 변환을 수행할 때 활성화뿐만 아니라 (가중치와 편향의) 파라미터에 대한 함수들을 사용한다. 다른말로 하자면, RELU/POOL 레이어는 고정 함수를 실행한다. CONV/FC 의 파라미터들은 gradient descent로 인해 학습이 될 것이다. 그렇게 학습을 함으로써, ConvNet이 계산한 클래스 스코어는 트레이닝 데이터 셋에 있는 각 이미지의 클래스 라벨과 동일해야 한다.

요약을 하자면:

- ConvNet 구조는 이미지 볼륨을 어떤 아웃풋 볼륨 (예, 점수 값 보유)으로 변환시켜주는 레이어들의 조합 중 가장 단순한 예시이다.
- 레이어에는 몇가지 기억해둘 타입이 있다: ConV/FC/Relu/Pool이 그 중 제일 유명하다.
- 각 레이어는 3차원 인풋 볼륨을 받고, 미분 함수를 통해서 3차원 아웃품 볼륨을 만든다.
- CONV/FC는 파라미터가 있지만, RELU/Pool은 없다.
- 어떤 레이어는 추가적으로 초파라미터를 가지고 있다. (CONV/FC/Pool은 yes, Relu는 no..)

![이미지](http://cs231n.github.io/assets/cnn/convnet.jpeg)

> ConvNet 구조 에시의 활성화를 보여주는 그림이다. 초기 불륨은 원본 raw 이미지 픽셀 (좌) 을 보유하고 있고, 마지막 볼륨은 클래스 스코어 (우)를 가지고 있다. 일련의 과정을 따라 활성화되는 각 볼륨은 여기서 한 열로 보여지고 있다. 마지막 레이어의 볼륨은 각 클래스에 대한 스코어를 가지고 있지만 이 예시에서는 5개의 고득점 클래스만 시각화했다. 본 웹 데모의 완성판은 우리 웹사이트에서 볼 수 있다. 여기저 보여주는 구조는 작은 VGG 넷 (나중에 설명할 거다)을 쓰고 있다.

우리는 이제 각 레이어와 그들의 하이퍼 파라미터 및 연결고리에 대해서 자세히 알아볼 거다.

### Convolutional 레이어

- Conv 레이어는 Conv 네트어크에서 대부분의 무거운 계산을 책임지는 핵심 요소이다.

**뇌구조 이야기 없이 훑어보기**

우선 Conv Layer에 대해서 뇌구조 비유법은 집어치우고 이야기를 해보자. ConV 레이어의 파라미터는 학습가능한 필터의 집합으로 이루어져 있다. 각 필터는 폭과 높이로 따졌을 때는 작지만 인풋 볼륨 깊이만큼 확장된다/늘어난다. 예를 들어, 보통 첫번째 레이어에 적용하는 필터는 5x5x3 (폭x길이x칼라채널개수이자-깊이)사이즈를 가졌다. forward 계산을 할 때, 각 필터를 인풋 볼륨의 폭과 높이에 대해 슬라이드 시킨다 (더 정확히는 말아 간다 라는 단어가 맞다) 그리고 필터의 값과 인풋 값을 내적한다. 이 슬라이딩 과정에서 우리는 2차원의 활성화 매핑값을 만드는데 각 위치에서 (5x5 마다의 위치) 필터와의 매핑값 결과가 담겨있다.직관적으로 생각한다면, 네트워크는 어떤 특정한 시각 정보를 발견했을 때 활성화되는 필터를 학습할 것이다. 초반의 레이어에서는 엣지(테두리 선)의 방향이나 색깔 뭉탱이를 보다가 나중에는 벌집 모양 전체 혹은 바퀴 모양의 패턴 등을 특징으로 발견할 것이다. 이제 우리는 각 ConV 레이어에 적용한 전체 필터 집합/모음을 가지게 되고 (12 필터), 각 필터는 각기 다른 2차원 활성화 매핑값을 만든다. 우리는 이 활성화 매핑을 깊이를 따라 쌓아서 전체 아웃풋 볼륨을 만들 것이다.

**뇌과학 관점**

네가 정 뇌/뉴런 비유법을 원한다면 알려주지.. 3차원 아웃풋 볼륨의 각 *entry* 는 뉴런의 아웃풋이라고 생각해볼 수도 있다. 어떤 뉴런? 인풋 이미지에서 작은 부분 하나만 보고 좌우에 있는 모든 뉴런과 그 파라미터를 공유하는 아이일 것이다 (왜냐하면 그 숫자들은 같은 필터를 적용해서 나오는 결과값이기 때문에). 우리는 이제 뉴런들의 연결성, 공간에서의 배치도, 공유되는 파라미터들에 대해서 알아볼 것이다.

**지역적 연결성** (너와 나의 연결고리..)

이미지와 같은 고차원 인풋을 가지고 놀 때, 위에서 보앗듯이, 이전 볼륨의 모든 뉴런에다가 지금의 뉴런을 다 연결시키는 건 비실용적인 짓이다. 대신, 우리는 각 뉴런을 인풋 볼륨의 작은 부분만이랑 연결시키고 싶다. 이 연결고리의 spatial 범위(이미지의 폭x높이/가로x세로)가 뉴런(뉴런의 사이즈는 필터의 사이즈와 같다.)의 receptive field라 불리는 하이퍼 파라미터이다. 깊이 축 쪽의 범위는 항상 인풋 볼륨의 깊이와 같다. 이렇게 폭x높이 차원에 대하는 방식과 깊이를 대하는 방식이 다르다는 점, 이 비대칭의 성질은 두 번 강조해도 시원찮다. 연결고리는 폭x넓이에 대해서는 지역적이지만, 깊이에 대해서는 전체 깊이 사이즈와 연결되어 있다.

예시1. 32x32x3 사이즈의 인풋 볼륨이 있다고 하자. (CIFAR-10의 RGB 이미지가 그렇다.) receptive field가 (필터 사이즈와 같다) 만약 5x5 라면, ConV Layer의 각 뉵런은 5x5x3 사이즈 부분에 대한 가중치를 가져서 토탈 5x5x3 = 75(가중치) + 1(편향) = 76개의 파라미터를 갖는다. 이 때, 이미지의 깊이가 3이니, 깊이 축을 따라 뉴런의 연결고리가 확장되는 사이즈는 반드시 3이어야 한다.

예시2. 16x16x20 사이즈의 인풋이 있다고 생각하자. receptive field의 크기는 3x3이다. ConvLayer에 있는 모든 뉴런의 각각 3x3x20 = 180 개의 연결고리를 가지고 있다. 이때 저체 이미지의 3x3 폭x높이 사이즈 마다 한 뉴런이랑 매핑되어 있고, 한 뉴런의 깊이는 20이다.

![이미지](http://cs231n.github.io/assets/cnn/depthcol.jpeg)

> 좌: 빨강색이 인풋 볼륨, ConV의 첫 레이어에 있는 뉴런의 볼륨. 이 레이어의 각 뉴런은 인풋 볼륨의 폭x높이에 대해 지역적으로만 `(필터 사이즈 만큼씩만)` 연결되어 있다. 기억하자: 깊이를 따라 여러 뉴런들이 있고 (여기서는 5), 다 인풋의 같은 지역 부분을 바라보고 있다 - 깊이 열에 대한 설명은 아래에서 이야기하자.

> 우: 뉴럴 네트워크에서 나왔던 뉴런 계산과정은 바뀌지 않았다. 여전히 그들의 가중치들과 인풋의 내적을 계산하고 non-linearity (비선형)을 거친다. 하지만 그들의 연결고리의 범위만 지역으로 제한된 것이다.

**Spatial Arrangement. 공간적 배치** 

Conv 의 각 뉴런이 인풋 볼륨과 연결되어 있다고 몇번을 말했는지 모른다. 하지만 우리는 아직 얼마나 많은 뉴런들이 아웃풋 볼륨에 있는지 혹은 어떻게 배치되어 있는지는 이야기하지 않았다. 아웃풋 볼륨을 결정하는 세가지 파라미터는 다음과 같다: 깊이, 스트라이드(간격), 제로 패딩(0여백)

1. 아웃풋 볼륨의 **깊이** 는 하이퍼 파라미터이다. 깊이는 우리가 사용할 필터의 개수에 상응하는데, 각 필터는 인풋에서 각기 다른 부분을 보고 있다. 예를 들어, 첫 Conv 레이어가 원본 이미지를 인풋으로 받았다. 그리고 깊이 부분에 들어있는 뉴런들은 다양한 방향을 가진 엣지/색깔 뭉탱이들의 존재에 반응하며 활성화될 수도 있다. 우리는 이미지의 같은 로컬부분을 바라보고 있는 뉴런 뭉태기들을 깊이 열 이라고 지칭할거다. (어떤 인간들은 fibre라는 용어를 선호한다. 니맴니맴 내맴내맴)

2. 두번째, 우리는 반드시! 필터를 슬라이드시킬 기준이 되는 스트라이드(간격)를 정해놓고 시작해야 한다. 스트라이드가 1이면 우리는 필터들을 한 번에 한 픽셀씩만 움직인다 (슬라이드시킨다). 스트라이드가 2이면 필터는 한 번에 두 픽셀을 점프해서 다음 로컬 영역으로 넘어간다. (3혹은 그 이상으로 스트라이드 값을 정하는 경우는 드물다.) 이렇게 2씩 점프한다면 아웃풋 볼륨의 폭x높이를 더 작은 사이즈로 만들어 낸다.

3. 아래서 곧 보겠지만, 가끔 인풋 볼륨의 모서리를 따라서 패딩(여백)을 0값들로 채워주는 게 편할 때가 있다. 이 제로 패딩의 사이즈가 하이퍼 파라미터이다. 제로 패딩의 좋은 특징은 이걸 이용해서 아웃풋 볼륨의 spatial size (폭x높이) 사이즈를 조절할 수 있다는 것이다. (잠시 후에 알려주겠지만, 거의 대부분은 인풋 볼륨의 spatial 폭x높이 사이즈를 원본이랑 똑같이 유지할 수 있도록 패딩 사이즈를 준다. 그래야 인풋과 아웃풋의 폭x높이 사이즈가 같아지니까.)

인풋 볼륨 사이즈의 함수(W), Conv 레이어 뉴런들의 receptive field 사이즈(F), 적용한 스트라이드 값(S), 모서리에 붙여 쓴 제로 패딩(P)의 양 -- 우리는 이것들을 가지고 아웃풋의 spatial 사이즈를 계산할 수 있다. 얼마나 많은 뉴런이 끼워맞춰지는지를 알고 싶으면 (W-F+2P)/S + 1 수식을 이용하면 된다고 네 뇌를 설득해도 무방하다 (그냥 외워도 된다는 소리다.) 예를 들어, 7x7 인풋, 스트라이드 1, 패딩0, 3x3 필터를 쓴다면

`(7 - 3 + 2 x 0) / 1 + 1 = 4+1 = 5` 계산해서 5x5 아웃풋 사이즈를 얻는다.

스트라이드 2라고 가정하면, 3x3 아웃풋이 나온다. (이 대입 계산 정도는 각자 해보긔...)

![이미지](http://cs231n.github.io/assets/cnn/stride.jpeg)

> 공간 배치의 묘사도. 이 예시에서는 공간 차원이 하나, x 축밖에 없고, 하나의 뉴런당 필터 사이즈가 3, 인풋 사이즈는 5, 제로 패딩 1.

> 좌: 인풋에 스트라이드 1 이 적용된 뉴런 --> (5-3+2)/1 +1 =5 아웃풋 사이즈가 나왔다.

> 우: 스트라이드 값을 2를 줬더니, 3이 나왔다. 스트라이드 3값은 쓸수가 없다 - 인풋 볼륨에 깔끔하게 적용이 안되니까. 수식으로 생각해보면, (5-3+2)=4 는 3으로 나눌 수가 없짜나?

> 뉴런 가중치들은 [1,0,-1] (최 우측) 이고, 편향치는 제로라고 생각하자. 이 가중치들은 노랑 뉴런 하나하나에 다 적용된다. (파라미터 공유에 대해서는 아래에서 설명할거다.)

*제로 패딩 사용*

위 이미지의 왼쪽을 보면, 인풋의 차원이 5이고 아웃풋 차원도 5이다. 이게 가능할 수 있는 이유는 receptive fields가 3이었고 제로 패딩으로 1을 주었기 때문이다. 만약 제로 패딩을 쓰지 않았다면 아웃풋 볼륨의 공간 차원은 3이 되었을 것이다. 왜냐하면 원본 인풋에 그만큼밖에 뉴런들이 'fit'할 수 없기 때문이다. 일반적으로, 인풋과 아웃풋의 볼륨의 spatial 사이즈를 똑같게 해주기 위해 스트라이드가 1일 때는 제로 패딩을 P = (F-1)/2 만큼 준다. 이런 식으로 제로 패딩을 사용하는 게 굉장히 흔하며 더 자세한 이유들은 ConvNet 구조 부분에서 다루겠다.

*스트라이드 제약*

하이퍼파라미터를 spatial 배치를 할 때 서로 간의 제약이 있어야 한다는 걸 기억해라. 예를 들어, 인풋의 사이즈가 W = 10, 제로 패딩은 0, 필터 사이즈 F = 3이라면, 스트라이드 값으로 2를 쓸 수는 없다. 왜냐하면 (W-F+2P)/S + 1 = (10-3+0)/2 + 1 = 4.5 가 나오기 때문이다. 이렇게 정수가 아니면 인풋에 뉴런들이 깔끔하고 대칭적으로 'fit'하지 않는다. 그러므로, 이런 식의 하이퍼 파라미터 설정은 불가능하고, ConvNet 라이브러리가 예외 에러를 반환하거나 fit 하기 위해 나머지에 제로 패딩을 주거나, 인풋을 크랍하거나 할 수도 있다. ConvNet 설계 구조 섹션에서 보겠지만, 적절하게 ConvNet들 사이즈를 맞춰줘서 모든 디멘션이 딱 잘 맞아 떨어지게 하는 게 보통 머리아픈 일이 아니다. 이 때, 제로패딩을 사용하거나 레이어 디자인 가이드라인들을 사용하면 고민을 많이 해결할 수 있다. 

*현실 예시 적용*

2012년에 ImageNet 챌린지를 우승한 Krizhevsky 등 저자 논문의 구조를 보면 227x227x3 사이즈의 이미지를 인풋으로 받는다. 첫번째 Conv 레이어에서는 receptive field사이즈가 F=11, S=4, P=0인 뉴런들을 사용했다. (227-11)/4+1=55가 되니, Conv레이어는 96의 깊이를 가지고, Conv 레이어 아웃풋 볼륨은 55x55x96의 사이즈를 갖게 되었다. 각기 55x55x96 사이즈인 뉴런들은 인풋의 11x11x3 사이즈 영역에 연결되어 있었다. 여기서 각 깊이 열에 있는 96개의 뉴런들은 다같이 같은 11x11x3 영역 부분에 연결되어 있고, 당연히 가중치 값들은 다 다르다. 재밌는 부분은, 이 논문을 제대로 읽었다면 알텐데, 논문에서는 인풋 이미지들이 224x224라고 주장하는데, 그럴 리가 없는 게 (224-11)/4+1 는 아무리 해봐도 정수 값으로 떨어져서 계산이 나오지 않기 때문이다. 이 때문에 ConvNets를 아는 사람들은 헷갈려 했고, 실제로 무슨 일이 있었는지는 잘 모른다. 내가 생각하기엔 Alex가 논문에서 언급하지 않은 제로패딩3 을 추가적으로 사용하지 않았을까 싶다.

**파라미터 공유**

Conv Layers 에서 파라미터의 넘버를 조절하기 위해 파라미터 공유 전략을 사용한다. 위에서 말한 현실적인 예시를 사용하면, Conv 첫번째 레이어에 55x55x96 = 290,400 개의 뉴런들이 있고, 각 뉴런들은 11x11x3 = 363 가중치와 1 편향치를 가지고 있는 것을 알 수 있다. 다 합쳐보면 ConvNet 첫번째 레이어 단 하나에만 무려 290400x364 = 105705600 파라미터들이 존재한다. 딱봐도 겁나 많다.

알고 보니! 우리는 한 가지 합리적인 가정을 만듦으로써 파라미터들의 개수를 엄청 많이 줄일 수 있다. 만약 이미지의 한 특성(엣지/칼라 등)이 어떤 (x,y) 공간 위치에서 유용하다면, 그건 (x2,y2)다른 위치에서도 분명 유용할 것이다.

이러한 파라미터 공유 전략을 이용한다면, 우리 예시에 있는 첫번째 Conv 레이어는 이제 96 개의 가중치 집합 (한 깊이 슬라이스당 한 집합)을 가지게 된다. 총 합계는 96x11x11x3 = 34,848 개의 독자적인 가중치 값 혹은 34,944개의 파라미터 (가중치+편향) 개수. 그게 아니라면, 각 깊이 슬라이스에 들어있는 뉴런들, 다 합친다면 55x55개인 뉴런들이, 모두 같은 파라미터를 쓰게 될 것이다. 오차역전파법을 실제로 쓸 때는, 볼륨마다 들어있는 각 뉴런들이 각자의 가중치의 기울기를 계산할 텐데, 이 기울기들은 깊이 슬라이스들을 따라 다 합쳐져서, 결국에는 한 깊이 슬라이스당 하나의 가중치 집합을 업데이트 할 것이다. 

보아라! 하나의 깊이 슬라이스에 있는 모든 뉴런들이 만약 같은 가중치 벡터를 사용하고 있다면, Conv 레이어의 forward pass는 각 깊이 슬라이스에서 그 안에 있는 뉴런들의 가중치와 인풋 볼륨을 convolution으로 계산할 것이다. (그래서 이름이 Convolutional Layer). 이게 바로 가중치의 집합을 그냥 (인풋과 함께 말아감긴) 필터(혹은 커넬)이라고 흔히들 지칭하는 이유이다. 

![이미지](http://cs231n.github.io/assets/cnn/weights.jpeg)

> 예시 필터들은 Krizhevsky et al. 의 논문에서 학습된 것들이다. 96개 필터 각각이 11x11x3 사이즈를 가지고 있다. **개당 하나의 깊이 슬라이스에 55x55개의 뉴런들을 공유하고 있다.** 파라미터 공유 가정이 나름 합리적이라고 느껴지지 않는가? 어떤 이미지의 특정 위치에서 수평의 모서리/엣지를 발견하는게 중요하다면, 그 특징은 다른 위치에서도 왠지 중요해야 할 것만 같다 -- 이미지들의 translationally-invariant 구조 때문에. `(약간 원판불변의 법칙 같은 느낌적인 느낌..?)` 그러니까 Conv layer 아웃풋 볼륨의 모든 55x55 지점마다 수평 엣지를 발견하기 위해 학습을 또하고 또하고 또할 필요가 없는 것이다.  


기억하라! 파라미터 공유 가정이 항상 말이 되는 것만은 아니다. ConvNet에 넣을 인풋 이미지들이 특정한 위치에 센터를 가진 구조라면, 이미지의 한쪽에서와 다른쪽에서 얻는 특징들이 완전히 다를 것이다. 하나 유용한 예제는, 인풋들이 얼굴이고, 이미지의 중앙이 중심점인 경우이다. 너는 아마 눈, 머리카락 별로 다 다른 공간(폭x높이)에서 학습될 거라고 생각하겠지? 그런 경우에는, 그냥 파라미터 공유를 버리고 레이어를 단순하게 Locally-connected 레이어라고 부른다. 

**넘파이 예제들**

위의 이야기들을 좀 더 구체화하기 위해 같은 말을 코드와 예시로 풀어보자. 인풋 볼륨이 x 라는 넘파이 배열이라고 가정한다. 

(x, y)좌표에 있는 depth 열은 `X[x,y:]` 액티베이션이다.
depth 슬라이스, 혹은 depth 특정지점 d에서의 activation map, 은 `X[:,:,d]` 액티베이션 이다.

**Conv Layer 예시**

인풋 볼륨 x 가 X.shape: (11,11,4) 라는 모양을 가졌다고 생각하자. 그리고 zero padding=0, filter size =5, stride=2라고 생각한다. 그렇다면 아웃풋 볼륨은 spatial 사이즈가 `(11-5)/2+1 =4` , 즉 폭과 넓이가 4인 볼륨을 내뱉을 것이다. 아웃풋 볼륨 V에서의 액티베이션 맵은 다음과 같을 것이다. (몇개만 계산해서 보여준다.)

```
V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0
V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0
V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0
V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0
```

넘파이에서는 * 라는 오퍼레이션 기호가 배열끼리의 원소별 곱셈을 의미한다는 것을 기억해내야 한다. 또한, 가중치 벡터 W0는 그 뉴런에서의 가중치 벡터이고, b0 은 편향이다. 여기서, 필터사이즈는 5, 인풋볼륨의 깊이는 4이기 때문에, W0은 W0.shape: (5,5,4) 모양일 것이다. 각 점마다, 이전에 평범한 신경망에서 보았던 것처럼 내적을 계산하는 것이다. 그리고 보시다시피 우리는 같은 가중치와 편향 값들을 사용하고 있고, 폭을 따라서 차원이 2만큼씩 커지고 있다 (stride). 

아웃풋 볼륨에 두번째 액티베이션 맵을 구성한다면 이렇다.
```
V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1
V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1
V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1
V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1
V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1 (example of going along y)
V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1 (or along both)
```

여기서는 V의 두번째 깊이 차원의 인덱스(인덱스 값 1) 를 건드리고 있는 걸 볼 수 있다. 왜 이러는 거냐면, 우리는 지금 두번째 액티베이션 맵을 계산하고 있고, 다른 파라미터 셋(집합)인 W1을 쓰고 있기 때문이다. 이 예시에서는 간략하게 표현하기 위해 Conv Layer의 다른 오퍼레이션을 생략했는데, 그 생략된 오퍼레이션 안에는 V아웃풋 배열의 다른 부분들의 계산 수행이 포함되어 있을 것이다. 추가적으로, 액티베이션 맵들은 ReLU와 같은 액티베이션 함수에 원소별로 투입된다. 그렇지만 여기다가는 안썼지롱.

**요약** 

Conv Layer를 요약해보자.

- W1 x H1 x D1 볼륨사이즈를 인풋으로 받는다.
- 네 종류의 하이퍼 파라미터가 필요하다
   - 필터의 개수 K
   - sptial (폭x높이) 범위 F
   - stride S
   - zero Padding 개수 P
- W2 x H2 x D2 아웃풋 볼륨 사이즈를 만들어 낸다. 
    - `W2 = (W1 - F + 2P)/S +1`
    - `H2 = (H1 - F + 2P)/S +1`
    - 폭과 높이는 대칭적으로 똑같이 계산된다.
    - D2 = K
- 파라미터 공유에 의해, 각 필터마다 `FxFxD1` 의 가중치 개수를 가지고, 총 `(FxFxD1)xK` 가중치와 K 편향 개수를 가진다.
- 인풋 볼륨, stride S에 대해서 d번째 필터로 컨볼루션을 하고 d번째 편향으로 상쇄시켜준 결과가 아웃풋 볼륨에서 d번째 깊이 슬라이스 (W2xH2 부분의 깊이 차원)이다.

하이퍼파라미터는 보통 Filter = 3, Stride =1, zeroPadding =1 이렇게 잡고 시작한다. 하지만, 이런 하이퍼 파라미터들을 조정하는 몇 가지 규칙같은 것이 있다. 아래의 ConvNet Architectures 단락을 보면 될 것.

**컨볼루션 데모**

아래는 ConV Layer가 돌아가고 있는 모습이다. 3차원 볼륨들은 시각화하기 어렵기 때문에 모든 볼륨들 (인풋은 파랑, 가중치는 빨강, 아웃풋은 초록)은 각 depth 슬라이스가 행 단위로 쌓여 있도록 표현했다. 인풋 볼륨은 사이즈가 W1 =5, H1=5, D1 = 3 이고, Conv Layer 파라미터들이 K =2 ,F=3, S=2, P=1이다. 이 말은, 우리가 2개의 필터를 가지고 있고, 각 필터의 사이즈는 3x3이며, Stride로 2를 적용할 것이라는 소리다. 그래서 아웃풋 볼륨 사이즈는 (5-3+2)/2+1 =3 의 폭x높이 사이즈를 같게 된다. 또한 패딩=1 이 인풋 볼륨에 적용되어 인풋 볼륨의 바깥 모서리들을 0으로 만들었다. 이 시각화 데모는 아웃풋 액티베이션 (초록)을 반복하며 돌아가는데, 그때 그때 하이라이트되는 아웃풋 원소는 -- 하이라이트된 파랑 인풋 지역 * 빨강 필터 들의 원소별 곱을 다 더한 후, 편향까지 적용한 결과이다.

**행렬 곱 연산 적용하기 implementation as matrix multiplication**

컨볼루션 연산은 곧 인풋의 로컬 영역과 필터 간의 내적을 계산하는 것이다. 일반적으로 CONV 레이어를 실행하는 패턴은 이렇게 컨볼루션이 내적연산이라는 점을 활용해서 컨볼루션 레이어의 forward 패스를 하나의 큰 매트릭스 곱으로 구성하는 것이다.

1. 인풋 이미지의 로컬 영역들은 im2col 이라는 연산을 통해서 '열'들로 늘려져서 나온다. (즉 박스같이 생긴 애들이 하나의 긴 일렬로 stretch 되어서 나온다는 이야기이다) 예를 들어, 인풋이 227x227x3이고, 11x11x3 필터 & 스트라이드 4 와 컨볼루션을 진행한다면, 11x11x3 사이즈의 픽셀 블락들을 인풋에서 꺼내와서 각 블락을 11x11x3 = 363 의 사이즈를 가지는 하나의 열 벡터로 늘여준다. stride 4 마다 이 과정을 인풋에 반복한다면, 폭x높이에 대해서 (227-11)/4+1 = 55 개의 지점들을 가지게 되고, im2col 의 아웃풋 행렬 X_col 로 336x3025 사이즈를 가지게 된다. 이 때 각 열은 일렬로 세워진 receptive filed 이며, 총 55x55 = 3025 개가 있다. receptive field 들은 겹치기 때문에, 인풋 볼륨의 넘버들이 여러 다른 열 벡터에 중복되어 들어갈 수가 있다.

2. CONV 레이어의 가중치들은 위와 비슷하게 행 벡터들로 늘어난다. 예를 들어, 11x11x3 사이즈를 가진 96개의 필터들이 있다면, 96 * 363 사이즈인 W_row 행렬을 반환할 것이다.

3. 컨볼루션의 결과는 이제 하나의 큰 매트릭스 곱과 같다. `np.dot(W_row, X_col)`
이는 각 필터와 각 receptive field 지점들의 내적이다. 우리의 예시에서는, 이 연산의 아웃풋이 96x3025일 것이며, 각 로케이션 x 각 필터의 내적 결과를 반환하는 것이다.

4. 최종 결과는 반드시 제대로 된 아웃풋 [55x55x96]의 차원으로 reshape 되어야 한다.

이 접근법은 메모리를 매우 많이 써야 한다는 단점이 있다. 인풋 볼륨의 어떤 값들은 X_col 에 중복되어 입려될 수 있기 때문이다. 하지만 매우 효율적으로 행렬 곱 연산을 수행하는 부분들이 있다는 게 장점이다. 그리고 여기서 말한 im2col 아이디어는 (우리가 곧 이야기할) 풀링 연산 수행에도 재사용될 수 있다.

**Backpropagation**

컨볼루션 연산의 backward 패쓰 (데이터와 가중치 모두에 대해서 backward) 또한 컨볼루션인데 필터들은 spatially 폭x높이가 반전되어 있다. 간단한 1차원 예시를 활용하면 쉽게 유도해 낼 수 있다.

**1x1 convolution**

그냥 해주는 말인데, [Network in Network](https://arxiv.org/abs/1312.4400)를 시작으로 여러 논문에서 1x1 컨볼루션을 사용한다. 어떤 사람들은 1x1 컨볼루션을 쓰는 것을 이상하게 생각했는데, 신호 처리 분야에서 온 사람들은 더욱 그랬다. 보통 신호들은 2차원이기 때문에 1x1 컨볼루션은 말이 되지 않는다 (pointwise scaling 이다). 하지만 ConvNets에서는 이야기가 다르다. 왜냐하면 우리는 3차원의 볼륨을 연산하고 있고 필터들은 항상 인풋 볼륨의 깊이 전체로 확장되어 훑고 지나가기 때문이다. 예를 들어, 인풋이 32x32x3이면, 1x1 컨볼루션은 3차원 내적 (인풋 깊이는 3 채널 수 이다)을 효율적으로 할 수 있다.

**Dilated Convolutions**

최근 발전된 아이디어는 CONV 레이어에 dilation이라고 불리는 새로운 파라미터 한 개를 추가하는 것이다. 여지껏 우리는 CONV 필터들이 연속적이라고만 얘기했다. 하지만 각 cell 사이마다 공간이 비게끔 필터를 줄 수도 있다. 이게 dilation이다. 예를 들어, 1차원에서, 사이즈 3을 가지는 필터 w를 인풋 x에 계산해보자: `w[0]*x[0] + w[1]*x[1] + w[2]*x[2]` 여기서 dilation은 0이다. dilation이 1이 되려면 필터가 `w[0]*x[0] + w[1]*x[2] + w[2]*x[4]` 이렇게 계산 될 것이다. 각 곱 사이사이에 1의 갭이 있는 걸 볼 수 있다. 어떤 상황에서는 이 아이디어를 0-dilated 필터들과 함께 쓸 때 매우 유용할 수 있다. 왜냐하면 더 적은 레이어들을 가지고 인풋에 뿌려져 있는 spatial 정보를 마구 긁어 모을 수 있기 때문이다. 예를 들어, 3x3 CONV 레이어 두개를 상하로 쌓는 다면, 두번째 레이어의 뉴런들이 5x5 인풋에 패치되는 함수라고 생각할 수 있다. (이 뉴런들의 효과적인 receptive field가 5x5라고 생각하자.) 만약 우리가 dilated 컨볼루션을 쓰면 이 receptive field가 더 빠르게 커질 것이다.

### Pooling Layer

ConvNet 설계에서는 연속적인 Conv 레이어들 사이 사이에다가 풀링 레이어를 주기적으로 끼어 넣어 주는 게 흔한 일이다. 이 풀링 함수는 점차적으로 (representation: 우리가 건드리는 이미지, 인풋부터 각 연산 이후의 이미지 본체들) spatial 사이즈를 줄여준다. 그 이유는 파라미터의 개수와 네트워크 연산 횟수를 줄임으로써 오버피팅을 조절하기 위해서다. 풀링 레이어는 독립적으로 인풋의 각 depth 슬라이스에서 MAX 연산을 실행하고, 그렇게 sptial 사이즈를 재조정한다. 가장 흔한 형태의 풀링 레이어는 필터 사이즈 2 * 2 와 스트라이드 2 를 downsamples (샘플 사이즈를 줄인 것들 일컫는 말)에 적용한 형태이다. 이 때, downsamples는 이미 사이즈를 한번씩 줄인 것들인데, 어떻게 줄인 거냐면: 각 깊이 슬라이스들의 폭 * 높이 간격이 2만큼씩 떨어져 있다. `즉 1:1:1, 3:3:2, 5:5:3. 이런식 -- 폭/높이는 2만큼씩 달라지고 깊이 슬라이스는 차례로. 맞는지 아닌지 모르겠...` 어쨌든, 이 흔한 풀링 형태는 75%의 액티베이션을 버린다.

**The most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations.**

이 경우에, 각 MAX 연산은 4개의 숫자에서 최댓값을 고른다. `little 2x2 region in some depth slice: 어떤 depth 슬라이스 에서는 작은 2x2 영역이다? 4개 숫자에서 고른다는건 이미 2x2 아닌가..? why saying little?` depth 차원은 바뀌지 않고 그대로 유지된다. 

일반적으로 풀링 레이어를 이야기하자면...:

- W1 x H1 x D1 볼륨 사이즈를 인풋으로 받는다.
- 두개의 하이퍼 파라미터가 필요하다
    - spatial 영역을 훑어갈 F
    - 스트라이드 (간격) S
- W2 x H2 x D2  볼륨 사이즈를 만들어 낸다:
    - W2 = (W1 - F)/S + 1
    - H2 = (H1 - F)/S + 1
    - D2 = D1
- 제로 파라미터라는 게 나오는데, 인풋의 고정 함수를 연산하기 때문이다 `???` *Introduces zero parameters since it computes a fixed function of the input*
- 풀링 레이어에다가는 제로 패딩을 쓰는게 흔하지 않다는 건 알아두길.

맥스 풀링 레이어를 쓸 때 보통 두 가지 방법 중 하나를 쓴다는 걸 알아두면 좋을 것이다. 풀링 레이어 F=3, S=2 (오버래핑 풀링 이라고 불린다. -겹치는-overlapping pooling), 더 자주 쓰이는 건 F=2, S=2. 큰 receptive field 사이즈를 가지는 풀링 사이즈는 너무 파괴적이다. (너무 영향 범위를 크게 잡으면 정보를 많이 잃을테니까. 4개중에서 1개 고르는 거랑 10개 중에서 한개 고르는 거랑, 나중에 아웃풋에서 정보를 찾으려면 아무래도 3개 버린게 9개 버린 것보다 정보가 많겟지?)

**General Pooling  일반적 풀링**

맥스 풀링말고도 더 알려주자면, 풀링 유닛(단위)들은 맥스가 아닌 평균(average) 혹은 L2-norm 등 다른 함수를 가질 수 있다. Average pooling은 역사적으로 자주 쓰여왔지만, 요즘은 맥스 풀링의 성능이 더 좋아서 잘 안쓰인다.

![이미지](http://cs231n.github.io/assets/cnn/maxpool.jpeg)

> 풀링 레이어는 인풋 볼륨의 각 depth 슬라이스에서 spatial 사이즈를 줄여준다.

> 좌: 이 예시에서, 인풋 볼륨 사이즈는 224x224x64이고, 필터 사이즈 2, 스트라이드 2로 풀링을 해서, 아웃풋으로 112x112x64 볼륨 사이즈를 뱉어낸다. 이 때 주의해서 볼 것은 볼륨 깊이는 (64) 그대로 라는 것.

> 우: 가장 흔한 downsampling 연산은 max pooling이고 여기서는 스트라이드에 2를 주었다. 그 말은, 2x2 사각형, 즉 4개의 숫자에서 1개의 최대값을 뽑는다는 소리이다.

**Backpropagation 오차역전파법**

backpropagation 챕터를 기억해보자. backward pass 할때 max(x,y)연산이 가지는 의미( *backward pass for a max operation* ??? )는 이렇게 해석할 수 있다. forward pass를 했을 때 가장 컸던 값인 인풋만 기울기 계산을 해서 뒤로 보낸다는 것이다. (routing: 신호를 보낸다의 의미로 생각). 그래서 풀링 레이어의 forward pass 과정에서는 max activation (swtiches라고도 부른다)의 인덱스 정보를 계속 킵해서, 오차역전파법을 할 때 gradient routing를 효율적으로 하는 게 일반적이다.

**Getting rid of pooling 풀링없애버리기**

많은 사람들이 풀링 연산을 싫어하고 그거 없이도 잘 할 수 있다고 생각한다. 예를 들어, [String for Simplicity_컨볼루션 간단하게 좀 하고싶다요](http://arxiv.org/abs/1412.6806) 논문에서는 풀링레이어 없애고 반복적인 CONV 레이어만 가지는 설계구조를 제시한다. 이 논문은 representation(이미지)의 사이즈를 줄이기 위해, CONV 레이어에서 큰 Stride 값을 가끔씩 주자고 제안한다. 풀링 레이어를 없애는 건 좋은 generative 모델들(variational autoencoders, gan 등)을 트레이닝 할 때 중요하다는 것도 발견되었다. 미래의 설계구조들은 아마 거의 혹은 아예 풀링 레이어를 안 쓸수도 있을 것 같다.

## Normalization Layer
사람들은 ConV-Net 설계에 쓰기 위해 많은 형태의 normalization 레이어들을 제안해왔다. 가끔은 진짜 뇌에서 발견한 inhibition scheme (숨바꼭질 전략)를 적용해보기 위한 의도도 있었다. 하지만, 이런 레이어들은 죽죽 인기를 잃었는데, 왜냐하면 실제로 적용했을 때 기여도가 매우 낮거나 없었기 때문이다. 다양한 normalization 종류에 대해서 자세히 알고싶으면 [cuda-convnet 라이브러리 API](http://code.google.com/p/cuda-convnet/wiki/LayerParams#Local_response_normalization_layer-same_map)를 참고해라.

### Fully-connected Layer (싸그리 다 연결)

일반적인 신경망에서 보았듯이, fully-connected 레이어에 있는 뉴런들은 이전 레이어의 모든 액티베이션들과 완전하게 연결되어 있다. 그래서 그들의 액티베이션 `뉴런과 액티베이션의 연산: 그럼 액티베이션이란 그냥 보통 연산을 생각하면 되는가??`) 은 행렬 곱으로 연산될 수 있고, 그 다음에 편향을 더한다. 더 자세한 것은 이전 페이지의 [Neural Networks](http://cs231n.github.io/neural-networks-1/)를 참고해라.

### Converting FC layers to CONV layers: FC 레이어에서 CONV 레이어로 바꾸기

FC와 CONV의 차이점은 단 하나: CONV 레이어의 각 뉴런은 인풋에 부분적인 영역에만 연결되어 있고, CONV 볼륨의 많은 뉴런들이 파라미터를 공유한다는 것이다. 하지만, Fc, Conv 모두 뉴런들은 내적 연산을 하고, **functional form** `기능???해석이...` 은 동일하다. 그래서 FC와 CONV 레이어들을 서로 변환할 수 있는게 사실상 가능하다는 것을 발견했다.

- 어떤 CONV 레이어든, 똑같은 forward 연산을 실행하는 FC 레이어를 찾을 수 있다. 가중치 행렬은 큰 사이즈의 행렬일텐데, 부분적인 블록 (로컬하게 연결되어 있으니까)만 빼고 대부분 값이 0일 것이며 블록 내부의 가중치들의 값들은 비슷비슷할 것이다 (파라미터 공유 특성때문에).
- 반대로 말하자면, 그 어떤 FC 레이어도 CONV 레이어로 바꿀 수 있다. 예를 들어, 인풋 볼륨 사이즈가 7x7x512이고, K=4096 (K는 필터/커널 사이즈)인 FC 레이어가 있다. 얘에 상응하는 CONV레이어를 찾는다면 F=7, P=0, S=1, K=4096일 것이다. 다른 말로 하자면, 필터 사이즈는 인풋 볼륨의 사이즈와 동일해야 하고, 아웃풋은 결국 1x1x4096 일 것이다. 왜냐하면 인풋 볼륨에 맞는 depth 열이 딱 하나밖에 없기 때문이다. (???) 그래서 이렇게 CONV로 바꾼애도 처음의 FC 레이어와 같은 결과를 내뱉는 것이다.

**FC -> CONV conversion**

이 쌍방 변환 중에서, FC레이어를 CONV 레이어로 바꿀 수 있다는 점이 매우 유용하다. ConVNet 구조가 224x224x3 의 이미지를 인풋으로 받는다고 치자. 연속적인 CONV 레이어들, Pool 레이어들을 사용해서 이미지를 7x7x512 의 액티베이션 볼륨 사이즈로 줄였다. (나중에 같이 볼 거긴 한데, AlexNet 구조에서는 이 때 5개의 풀링 레이어들로 인풋을 spatial 하게 매번 1/2씩 downsample해서, 마지막 sptaial size는 224 / 2 /2 /2 /2 /2 = 7 이 나왔다.) 그리고 여기에서, AlexNet은 4096 사이즈인 FC 레이어 2개를 썼고, 마지막 FC 레이어들 `last FC layers? 1 or 2?? -s는 오타인듯?` 은 1000개의 뉴런들로 클래스 스코어를 계산했다. 우리는 이 3개의 FC 레이어들을 각각 CONV 레이어로 바꿀 수 있다: 아래처럼...

- 7x7x512의 볼륨을 가진 첫번째 FC 레이어를 --> 필터 F=7인 CONV layer로 바꿔서, 1x1x4096 아웃풋 볼륨을 만든다.
- 두번째 FC 레이어는 F=1인 ConV레이어로 바꿔서 1x1x4096을 만들자.
- 마지막 FC 레이어도 F=1인 ConV로 바꿔서 최종 아웃풋은 1x1x1000이다.

이 각각의 변환들은 현실적으로 각 FC 레이어에 있는 가중치 행렬 W를 ConV 레이어 필터로 조작해주는 (예를 들어, reshaping) 과정이 필요할 수도 있다. 우리는 이 변환을 이용해서 단 한번의 forward pass를 하면서, ConvNet을 (큰 이미지의) 수많은 spatial 위치들에 대해 효과적으로 슬라이드시킬 수 있다는 게 좋은 발견 포인트이다.

예를 들어, 224x224이미지가 7x7x512의 볼륨 사이즈를 준다 `???` 고 생각해보자. 32, 즉 (224/7) 만큼 줄이고, 이미지 사이즈 384x384 를 바뀐 설계구조에다가 forward 패쓰한다. 이렇게 하면 12x12x512 볼륨 사이즈가 나올 건데, 384/32=12 이기 때문이다. 그 다음 3 개의 CONV 레이어들(우리가 방금 FC에서 ConV로 바꾼 애들)을 죽 따라가보면 마지막 최종 볼륨 사이즈는 6x6x1000 일 것이다. (12-7)/1 + 1 = 6 이니까. 클래스 스코어 값을 가진 단일(싱글) 벡터 91x1x1000 사이즈) 가 아니라, 우리는 최종적으로 384x384 이미지에 대해서 6x6 배열의 클래스 스코어를 가지게 되는 것이다.

> Evaluating the original ConvNet (with FC layers) independently across 224x224 crops of the 384x384 image in strides of 32 pixels gives an identical result to forwarding the converted ConvNet one time.

자연스럽게, 변환한 ConvNet을 forward 한번 하는 건 원래의 CONVNET을 모든 36개 지점에서 반복하는 것보다 훨씬 효율적이다. 왜냐하면 36번의 평가들은 연산을 어차피 공유할테니까 (비슷한 연산을 할꺼란 소리). 이 수법은 현실 예제에서 더 좋은 성능을 얻기 위해 쓰인다. 예를 들어, 이미지를 resize해서 더 크게 만들고, 변환한 CONVNET을 써서 클래스 스코어를 수많은 spatial 지점들에서 평가하고, 그 스코어들을 평균내는 건 흔한 방식이다.

마지막으로, 만약 우리가 효율적으로 오리지널 ConVNET을 이미지에 적용하고 싶은데 stride를 32픽셀보다 작은 걸 주고 싶다면? 우리는 여러번의 forward pass를 해서 이걸 해볼 수 있다. 예를 들어, 우리가 16픽셀의 스트라이드를 쓰고 싶다면, 변환한 CONVNET를 두 번 forwarding해서 받은 볼륨 값들을 합쳐서 그렇게 해볼 수 있다: 첫번째는 원본 이미지에 대해서 forward 연산, 두번째는 spatial 적으로 (폭, 높이에 대해서) 16픽셀만큼 shifted된 이미지에다가 연산한다.
- [이 파이썬 노트북은 Caffe로 변환 연산을 어떻게 수행했는지 코드로 보여준다.](An IPython Notebook on Net Surgery shows how to perform the conversion in practice, in code-using Caffe)

## ConvNet 구조

CNN 은 보통 세 가지의 레이어로 구성되어 있다: CONV, POOl, FC. 그리고 원소별로 non-linearity를 적용하는 RELU 액티베이션 함수를 레이어로 명시할거다. 이 섹션에서는 요것들이 CONVNET 전체를 이루기 위해 어떻게 쌓이는지 볼 것이다.

### Layer Patterns 레이어 패턴

가장 흔한 CONVNET 구조 모양은 CONV-RELU 레이어들을 몇 개 쌓은 다음 POOL 레이어를 쌓는 패턴을 반복하는 것이다. 이미지가 작은 사이즈로 spatial 하게 합쳐질 때까지 반복한다. 중간에 fully-connected 레이어들로 바꾸는 것도 흔한 방법이다. 마지막 FC 레이어는 클래스 스코어같은 아웃풋을 가지고 있다. 즉, 가장 흔한 CONVNET 구조는 다음과 같은 패턴을 따른다:

`INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC`

* 은 반복을 의미하고, POOL? 은 풀링 세이어를 선택적으로 집어넣을 수 있다는 말이다. 그리고 N >= 0, (주로 N<=3), M >=0, K>=0 (주로 K<3)이다. 예시로, 여기 흔한 CONVNET 구조 패턴들을 보여주겠다.

- `INPUT -> FC`, implements a linear classifier. `Here N = M = K = 0.`

- `INPUT -> CONV -> RELU -> FC`

- `INPUT -> [CONV -> RELU -> POOL]*2 -> FC -> RELU -> FC`  여기서는 CONV 레이어 하나를 POOL 사이사이마다 넣었다.

- `INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC`  여기서는 두개의 CONV 레이어를 쌓고, POOL 레이어 사이마다 넣었다. 크고 깊은 네트워크를 만들기 위해서는 이게 굉장히 좋은 방법이다. 그 이유는 겹겹이 쌓인 CONV 레이어는 인풋 볼륨의 더 복잡한 피쳐/특징들이 pooling 연산으로 뭉게지기 전에 잡아낼 수 있다.

**작은 필터 CONV 한 묶음이 큰 receptive field를 가진 CONV 레이어 하나보다 낫다**

세 개의 3x3 CONV 레이어를 쌓았다고 겹겹이 쌓았다고 하자 (이때, 이들 사이에 non-linearity 는 없다). 이 배열에서 첫번째 CONV 레이어의 각 뉴런들은 인풋 볼륨을 3x3만큼 보는거다. 두번째 CONV 레이어의 뉴런은 첫번째 CONV 레이어를 3x3 구멍만큼 본다. 즉. 인풋 볼륨의 5x5를 보는 거다. 비슷하게, 세번째 CONV 레이어의 뉴런은 두번째 CONV 레이어를 3x3만큼 보고, 인풋 볼륨의 7x7 을 보는 효과가 생긴다. 이렇게 3x3 CONV 세개짜리를 생각하지 말고, 7x7 receptive field 가진 CONV 레이어 하나를 본다고 가정하자. 얘에 있는 류너들은 인풋 볼륨의 spatial 부분을 7x7만큼 볼텐데, 몇가지 단점이 있다. 첫번재, 뉴런들은 인풋에 대해서 선형 함수 (linear function)를 실행한다. 반면 cONV 세겹은 non-linearies를 가지고 있어서 특징들을 더 돌출시켜준다. 두번째, 모든 볼륨이 C 채널개수를 가지고 있다고 하면, 7x7 CONV 레이어 한개는 C x (7x7xC) = 49C^2 파라미터개수를 가질 것이다. 반면 세겹의 3x3 CONV 레이어들은 3 x (C x (3x3xC)) = 27C^2 파라미터밖에 안 가진다. 직관적으로 알 수 있듯이, 여러 CONV 레이어를 쌓아서 작은 필터들을 사용하는 것이 큰 필터를 가진 한 개의 CONV 레이어 보다 인풋의 강려크한 피쳐들을 잘 살려낼 수 있다. 하지만 오차역전파법을 하게 되면 중간중간의 CONV 레이어들을 킵하기 위해 메모리가 더 필요다는 게 단점이다.

**최근 변화**

레이어들의 배열 패러다임이 최근 들어 반박되고 뒤엎어지고 있다는 것을 알아두자. 구글의 inception 구조, (현재 인기 최고인) 마소 아시아의 Residual Networks 들이 그 예이다. (아래 케이스 스터디 섹션에서 말할건데) 둘 다 더 복잡하고 다른 연결 구조들을 선보인다.

**현실 예제: ImageNet 에 잘 먹히는 걸 써라**

어떤 구조로 할지 결정 내리는 게 너무 어렵다면, 90%이상은 딱히 걱정하지 않아도 된다. 내 조언은 '최고가 되려하지마라'이다. 네가 직접 만든 구조를 써서 망치느니, 지금 ImageNet에 넣어서 제일 잘 돌아가는 애를 택하라는 것이다. 이미 트레이닝 된 모델을 다운 받고 네 데이터에 맞게 조정해라. CONVNET 을 처음부터 트레이닝 하거나 디자인할 필요는 없다. 이 얘기는 내가 [딥러닝스쿨](https://www.youtube.com/watch?v=u6aEYuemt0M)에서 말한 적이 있다.


**Layer Sizing Patterns 레이어 사이즈 패턴**

여태까지 ConvNet레이어에서 흔하게 쓰이는 하이퍼파라미터들은 설명을 따로 하지 않았다. 먼저 구조들을 어떻게 사이즈를 잡는지 등 기본적으로 많이 쓰이는 규칙들을 설명하겠다.

**인풋 레이어**

이미지를 가진 인풋 레이어는 2로 수없이 나눠질 수 있는 사이즈여야 한다. 보통 CIFAR-10처럼 32, 64, STL-10처럼 96, ImageNet ConvNet 처럼 224, 384, 512 등이 있다.

**conv 레이어**

3x3, 5x5같은 작은 필터들, stride는 1을 사용해야 되고, 특히 인풋 볼륨에 패딩은 0으로 줘야지 conv layer가 인풋의 spatial차원을 변형시키지 않을 것이다. 즉, F=3, P=1을 써서 인풋의 원본 사이즈를 유지해라. 대부분의 F 값을 가지고, P=(F-1)/2 로 P 값을 잡으면 인풋 사이즈를 유지할 수 있을 것이다. 만약 반드시 큰 필터 사이즈(7x7처럼)를 써야 한다고 하면, 인풋 이미지를 바로 받는 제일 첫번째 conv layer에다가만 이걸`이걸이 뭐지..` 쓰는 게 흔한 경우이다.

**pool layers**

풀링 레이어는 인풋의 spatial 차원의 사이즈를 축소하는 역할을 담당한다. 가장 흔한 세팅은 2x2 receptive fields (F=2)와 stride=2를 적용하는 맥스 풀링을 사용하는 것이다. 이렇게 downsampling을 하면 인풋 볼륨의 액티베이션을 75%나 없앤다 (폭과 높이에 대해 2만큼씩 downsampling하니까). 또다른 살짝 다른 흔한 세팅은 3x3 receptive fields와 stride=2이다. `그런데...하면서 말이 짤렸음` 3보다 큰 맥스풀링은 거의 찾아볼 수 없는데, 그렇게 하면 풀링으로 너무 많은 것을 잃어버리기 때문이다. 성능만 더 떨어뜨리는 격이 될 것이다.

**Reducing sizing headaches 사이즈 골칫거리 줄이기**

위에서 만한 방법들이 좋은 이유는 POOL 레이어만 downsampling(볼륨 중 폭과 높이에 대해서만)을 감당하고, 모든 CONV 레이어들은 인풋의 spatial 사이즈를 유지하기 때문이다. 1보다 큰 stride를 쓰거나 ConV레이어들의 인풋에 0 패딩을 주지 않는 방법에서는 인풋 볼륨의 사이즈를 CNN구조 내내 잘 따라댕기면서 체크해야 한다. 그러면서 우리가 주는 stride, filter가 잘 먹히는지, 즉 ConvNet 구조가 이쁘고 대칭적이게 연결되는지 봐야한다.

**why use stride of 1 in CONV? 왜 간격을 1 주나요?**

작은 스트라이드는 실제 구현할 때 잘 먹힌다. 그리고 이미 말했듯이 stride=1을 주면 spatial downsampling은 풀링 레이어만 담당하면 된다. Conv 레이어들은 인풋 볼륨의 깊이 차원만 바꿀 것이다.

**why use padding ? 패딩은 왜 쓰나요?**

앞에 말한 spatial 사이즈를 유지한다는 것 말고도, 이렇게 하면 성능도 개선되는 장점이 있다. 만약 CONV 레이어들이 인풋을 제로 패딩하는 것이 아니라 컨볼루션만 실행한다면, 각 CONV이후마다 볼륨들의 사이즈가 조금씩 줄을 것이고, 모서리에 있는 정보들은 금방 날라갈 것이다.

**compromising based on memory constraints 제한된 메모리와 타협하기**

어떤 경우 (특히 초기의 ConvNet 구조들)에서는, 위의 규칙/습관대로 하다가는 메모리가 금방 차버린다. 예를 들어, 224x224x3 이미지를 64개의 필터, 패딩 1를 가진 3x3 CONV레이어들에다가 필터링 시키면 3개의 액티베이션 볼륨 사이즈는 [224x224x64]가 된다. 이걸 총 합치면 천만 개의 액티베이션, 72MB 메모리 (한 이미지 당, 액티베이션과 그래디언트 합쳐서)의 양이다. 흔히 GPU들은 메모리에 의해 병목현상이 생기기 때문에 타협하는 수밖에는 없을 것이다. 실전에서는 사람들이 네트워크의 첫번째 CONV레이어에서만 타협하는 걸 선호한다. 예를 들어, 타협의 방법 중 하나는 첫번째 CONV 레이어에 7x7 필터 사이즈, stride=2 (ZF net처럼)를 주는 것이다. 또다른 예인 AlexNet은 필터 사이즈를 11x11, stride=4를 주었다.

### Case Studies

컨볼루션 네트워크 CNN 에서 이름을 남긴 여러 구조들이 있다. 가장 흔한 것들을 설명하겠다.

- **LeNet**: 1990년대 얀 르쿤은 처음으로 성공적인 Convolutional Networks의 응용작들을 만들었다. 가장 유명한 건, 우편 주소와 숫자들을 읽어내는 LeNet 구조이다.

- **AlexNet**: 컴퓨터 비젼 분야에서 컨볼루션 네트워크의 인기를 높인 첫번째 주인공은 Aelx Krizhevsky, Ilya Sutskever, Geoff Hinton의 AlexNet이다. 2012년에 Alexnet은 ImageNet ILSVRC 챌린지에 제출되었고, 2등과 에러율 10% 차이를 보여주며 뛰어난 성능을 보여줬다. (top 5 에러가 16%, 2등은 26%). 네트워크는 LeNet과 비슷하지만, 더 깊고 크다. 그리고 Convolutional 레이어들을 겹겹치 쌓는다 (이전에는 하나의 CONV 레이어와 POOL레이어를 사용하는 게 흔했다.)

- **ZF Net**: ILSVRC 2013 승자는 Matthew Ziler, Rob Fergus의 컨볼루션 네트워크였다. ZFNet으로도 불린다. AlexNet의 구조 하이퍼파라미터를 조금 바꿔서 개선시켰다. 특히, 중간의 컨볼루션 레이어들의 사이즈를 키웠고, 첫번째 레이어의 stride, 필터 사이즈를 줄였다.

- **GoogleNet**: ILSVRC 2014년 승자는 구글의 Szegedy외 다수의 컨볼루션 네트워크이다. 주요 기여도는 네트워크의 파라미터 수를 급격히 줄인 (AlexNet이 60million, 얘는 4million) Inception Module 인셉션 모듈의 개발이다. 추가적으로, 이 페이퍼는 ConvNet위에 FC 레이어를 주는 게 아니라, Average Pooling을 사용해서 별 도움 안되는 파라미터들을 많이 제거해버린다. 이 논문의 팔로우업/후속 버젼들도 있다. 최근에는 Inception-v4.

-**VGGNet**: ILSVRC 2014의 상위권에 들었던 또다른 네트워크는 Karen Simonyan, Andrew Zisserman의 네트워크, VGGNet이다. 얘의 주 기여도는 네트워크의 깊이가 중요한 성능 개선 요소라는 것을 알려주는 점이다. 이들의 최고 성능 네트워크는 16 CONV/FC 레이어들, 완전 다 똑같은 구조들로 이루어져있고 3x3 컨볼루션과 2x2풀링만을 처음부터 끝까지 수행한다. 얘네를 미리 트레이닝시킨 모델을 Caffe 에서 plug & play 용도를 사용 가능하다. (쉽게 여기저기 갖다 붙일 수 있다는 소리 같음) VGGNet의 단점은 평가하기가 비싸고 (비용/메모리/시간이 많이 든다), 메모리와 파라미터(140million)를 아주 많이 사용한다는 것이다. 이 파라미터의 대부분은 첫번째 FC 레이어에 있다. 이 FC 레이어들을 없애도 성능에는 큰 지장이 없고 필요한 파라미터 개수도 많이 지울 수 있다는 게 이후의 연구들을 통해 밝혀졌다.

-**ResNet**: Kaiming He외 다수에 의해 개발된 Residual Network는 ILSVRC 2015년에 1등했다. 얘는 특별한 skip connections 라는 건 사용했고, batch normalization을 중점적으로 사용했다. 얘는 네트워크의 마지막에 fc 레이어들도 없다. 독자는 Kaiming의 발표와 최근 연구들을 참고해서 torch에서 재현해볼수있다. ResNet은 거의 CNN에서 최고 인기와 성능을 보여주고 있고 (적어도 2016, 5,10까지는) ConvNet을 쓴다하면 이 모델이었다. 이 모델을 조금 바꾼 최근 발전사항들은 [여기서](https://arxiv.org/abs/1603.05027) 볼 수 있다. 

### Computational Considerations

## Additional Resources
